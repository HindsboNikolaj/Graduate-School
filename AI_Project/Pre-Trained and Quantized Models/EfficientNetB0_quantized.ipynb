{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import itertools\n",
        "import os\n",
        "import time\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "\n",
        "# 3rd party modules\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.quantization import quantize_dynamic\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1: Load the Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_model(output_classes=6):\n",
        "    \"\"\"\n",
        "    Initializes the ResNet-50 model pre-trained on ImageNet (just convolutional layers) and implements the final fully connected layer.\n",
        "\n",
        "    Parameters:\n",
        "    - output_classes (int): The number of classes for the final output layer.\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): The modified model based on the pre-trained Resnet-50 feature representation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load a pre-trained ResNet-50 model\n",
        "    model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
        "\n",
        "    # Freeze all the parameters in the model to prevent them from being updated during training\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Get the number of input features to the final fully connected layer\n",
        "    in_features = model.fc.in_features # 2048 input features (from the last feature mapping layer of Resnet)\n",
        "\n",
        "    # Replace the final fully connected layer with a new one that has the desired number of output classes\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(in_features, 512),  # Reduce dimension from in_features to 512\n",
        "        nn.ReLU(),                    # Apply ReLU activation function\n",
        "        nn.Linear(512, 6)  # Final layer with 'output_classes' number of outputs\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_model(model_path, output_classes=6):\n",
        "    # Initialize the model structure\n",
        "    model = initialize_model(output_classes)\n",
        "    \n",
        "\n",
        "    # Load the trained model weights\n",
        "    saved_contents = torch.load(model_path)\n",
        "    state_dict = saved_contents[\"state_dict\"]\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "model_path = 'RESNET50_best_model.pth'  # Adjust the path as needed\n",
        "model = load_model(model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2: Apply Quantization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): DynamicQuantizedLinear(in_features=2048, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "    (1): ReLU()\n",
              "    (2): DynamicQuantizedLinear(in_features=512, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()  # Ensure the model is in evaluation mode before quantization\n",
        "\n",
        "# Apply dynamic quantization\n",
        "quantized_model = quantize_dynamic(\n",
        "    model, \n",
        "    {nn.Linear},  # Specify the layer types to quantize\n",
        "    dtype=torch.qint8  # Specify the quantization data type\n",
        ")\n",
        "\n",
        "# Move the quantized model to the appropriate device if you are using GPU for evaluation\n",
        "device = torch.device(\"cpu\")  # Quantized models are typically used on CPUs\n",
        "quantized_model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 3: Evaluate the Quantized Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data(data_dir):\n",
        "    \"\"\"\n",
        "    Prepares the dataset for training/testing by organizing image paths and labels.\n",
        "\n",
        "    Parameters:\n",
        "    - data_dir (str): Directory containing the dataset, organized into subdirectories\n",
        "      for each category.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Two lists containing the image file paths and corresponding labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the categories of the dataset\n",
        "    categories = ['Trash', 'Plastic', 'Paper', 'Metal', 'Glass', 'Cardboard']\n",
        "\n",
        "    # Lists to hold the paths of the images and their labels\n",
        "    image_paths = []\n",
        "    labels = []  # Numerical labels: 0 for Trash, 1 for Plastic, etc.\n",
        "\n",
        "    # Enumerate over categories to label the images accordingly\n",
        "    for label, category in enumerate(categories):\n",
        "            try:\n",
        "                # Construct the directory path for the current category\n",
        "                category_dir = os.path.join(data_dir, category)\n",
        "\n",
        "                # Iterate through each file in the category directory\n",
        "                for file in os.listdir(category_dir):\n",
        "                    # Check if the file is an image\n",
        "                    if file.endswith('.jpg') or file.endswith('.png'):\n",
        "                        image_paths.append(os.path.join(category_dir, file))\n",
        "                        labels.append(label)  # Assign the label to this image\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process category {category} due to error: {e}\")\n",
        "                continue\n",
        "    return image_paths, labels\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class that extends PyTorch's Dataset class for image loading and preprocessing.\n",
        "\n",
        "    Attributes:\n",
        "    - image_paths (list): List of paths to the images.\n",
        "    - labels (list): List of labels corresponding to the images.\n",
        "    - transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset with images and labels.\n",
        "\n",
        "        Parameters:\n",
        "        - image_paths (list): List of paths to the images.\n",
        "        - labels (list): List of labels for the images.\n",
        "        - transform (callable, optional): Optional transform to apply on images.\n",
        "        \"\"\"\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves an image and its label from the dataset at the specified index.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the image and label to return.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "\n",
        "        image_path = self.image_paths[index]\n",
        "    \n",
        "        # Print the image path for debugging\n",
        "        # print(f\"Accessing image: {image_path}\")\n",
        "\n",
        "        # Check if the file exists\n",
        "        if not os.path.exists(image_path):\n",
        "            raise FileNotFoundError(f\"The file {image_path} does not exist.\")\n",
        "\n",
        "        # Attempt to open the image file\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            # If an error occurs, print the error and the problematic file path\n",
        "            print(f\"Error opening image at {image_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "        return image, label\n",
        "\n",
        "def evaluate_model(model, dataloader, loss_fun, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The neural network model to evaluate.\n",
        "    - dataloader: The DataLoader containing the dataset for evaluation.\n",
        "    - loss_fun: The loss function used to compute the model's loss.\n",
        "    - device: The device (CPU or CUDA) on which the computations will be performed.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple of average loss and accuracy over the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode    test_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed for evaluation\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)  # Move data to the same device as the model\n",
        "\n",
        "            pred = model(x)\n",
        "            _, predicted_classes = torch.max(pred, 1)\n",
        "            correct_predictions = (predicted_classes == y).float()\n",
        "\n",
        "            loss = loss_fun(pred, y.long())  # Ensure consistent data type\n",
        "            val_loss += loss.item()\n",
        "            val_acc += correct_predictions.sum().item() / y.size(0)\n",
        "\n",
        "        val_loss /= len(dataloader)\n",
        "        val_acc /= len(dataloader)\n",
        "\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TEMPORARY\n",
        "folder_path = '../../data/dataset-resized' # My local path\n",
        "image_paths, labels = prepare_data(folder_path)\n",
        "batch_size = 32\n",
        "# Split the data into training and test sets\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.15, random_state=0)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    train_paths, train_labels, test_size=0.175, random_state=0)  # 0.175 x 0.85 ~= 0.15\n",
        "\n",
        "# Normalization parameters found to be optimal for CIFAR dataset; may need adjustment for other datasets\n",
        "normalization = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "# Define the training data transformations\n",
        "training_transform = transforms.Compose([\n",
        "    transforms.Resize(256, antialias=True),  # Resize images to 256x256, with antialiasing\n",
        "    transforms.RandomCrop(224),  # Crop randomly to 224x224 for data augmentation\n",
        "    transforms.RandomRotation(20),  # Rotate images up to 20 degrees for data augmentation\n",
        "    transforms.RandomHorizontalFlip(0.1),  # Horizontally flip images with a probability of 0.1 for data augmentation\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),  # Randomly adjust color settings\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.1),  # Randomly adjust sharpness for data augmentation\n",
        "    transforms.ToTensor(),  # Convert images to tensors\n",
        "    transforms.Normalize(*normalization),  # Normalize images using the specified mean and std dev\n",
        "])\n",
        "\n",
        "# Define the validation and test data transformations (simpler than training transformations)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256, antialias=True),  # Resize images to 256x256, with antialiasing\n",
        "    transforms.CenterCrop(224),  # Center crop images to 224x224\n",
        "    transforms.ToTensor(),  # Convert images to tensors\n",
        "    transforms.Normalize(*normalization),  # Normalize images using the specified mean and std dev\n",
        "])\n",
        "\n",
        "# Create dataset objects for training, validation, and testing\n",
        "train_dataset = CustomDataset(train_paths, train_labels, transform=training_transform)\n",
        "val_dataset = CustomDataset(val_paths, val_labels, transform=transform)\n",
        "test_dataset = CustomDataset(test_paths, test_labels, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized Model Test Loss: 0.5621, Test Accuracy: 0.8341\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a function to create your dataloaders\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Evaluate the model\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "test_loss, test_acc = evaluate_model(quantized_model, test_dataloader, loss_fun, device)\n",
        "\n",
        "print(f\"Quantized Model Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
