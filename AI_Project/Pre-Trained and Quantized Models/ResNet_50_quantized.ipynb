{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import itertools\n",
        "import os\n",
        "import time\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "\n",
        "# 3rd party modules\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torch.quantization import prepare, convert\n",
        "from torch.quantization import fuse_modules, QuantStub, DeQuantStub\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.quantization import quantize_dynamic\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1: Load the Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_model(output_classes=6, quantize = True):\n",
        "    \"\"\"\n",
        "    Initializes the EfficientNet-B5 model with a custom final layer for the given number of output classes.\n",
        "\n",
        "    Parameters:\n",
        "    - output_classes (int): The number of classes for the final output layer.\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): The modified model based on the pre-trained EfficientNetB5 feature representation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load a pre-trained EfficientNet-B5 model from the timm library\n",
        "    model = timm.create_model('efficientnet_b5', pretrained=True)\n",
        "\n",
        "    # # Freeze all the parameters in the model to prevent them from being updated during training\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = False\n",
        "    ## This was removed in part from ablation study, because the water bottle classification was making plastic heavy in prediction for the model.\n",
        "\n",
        "\n",
        "    # Get the number of input features to the final fully connected layer\n",
        "    # The classifier layer is the final layer in EfficientNet models\n",
        "    in_features = model.classifier.in_features\n",
        "\n",
        "    # Replace the final classifier layer with a new one that has the desired number of output classes\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Linear(in_features, 512),  # Reduce dimension from in_features to 512 (from 1280 in this case)\n",
        "        nn.ReLU(),                    # Apply ReLU activation function\n",
        "        nn.Linear(512, output_classes)  # Final layer with 'output_classes' number of outputs\n",
        "    )\n",
        "\n",
        "    if quantize:\n",
        "        model.quant = QuantStub()\n",
        "        model.dequant = DeQuantStub()\n",
        "\n",
        "    return model\n",
        "\n",
        "def fuse_model(model):\n",
        "    \"\"\"\n",
        "    Applies module fusion to optimize layers of a given model for quantization. Fuses Convolution, BatchNorm,\n",
        "    and ReLU layers where applicable.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The model to be fused.\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): The model with fused layers.\n",
        "    \"\"\"\n",
        "    for module_name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
        "        module = getattr(model, module_name)\n",
        "        for submodule_name, submodule in module.named_children():\n",
        "            if 'conv' in submodule_name:\n",
        "                fuse_modules(submodule, ['conv1', 'bn1', 'relu'], inplace=True)\n",
        "                if hasattr(submodule, 'downsample'):\n",
        "                    fuse_modules(submodule.downsample, ['0', '1'], inplace=True)\n",
        "    return model\n",
        "\n",
        "class QuantizedResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A wrapper class for the quantized ResNet model that includes quantization and dequantization steps in the forward pass.\n",
        "\n",
        "    Attributes:\n",
        "    - model (torch.nn.Module): The base model with added quantization and dequantization modules.\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        super(QuantizedResNet, self).__init__()\n",
        "        self.model = model\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Processes input through the model's quantization, base layers, and dequantization.\n",
        "\n",
        "        Parameters:\n",
        "        - x (Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "        - x (Tensor): The output tensor after processing.\n",
        "        \"\"\"\n",
        "        x = self.model.quant(x)\n",
        "        x = self.model(x)\n",
        "        x = self.model.dequant(x)\n",
        "        return x\n",
        "\n",
        "def static_quantize(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Applies static quantization to a model. This process includes inserting observers,\n",
        "    calibrating the model with a calibration dataset, and converting the model to use quantized weights.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The model to quantize.\n",
        "    - dataloader (DataLoader): The DataLoader for the calibration dataset.\n",
        "    - device (torch.device): The device to perform quantization on.\n",
        "\n",
        "    Returns:\n",
        "    - model_int8 (torch.nn.Module): The quantized model.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    model_fp32_prepared = prepare(model)\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs, _ = batch\n",
        "            inputs = inputs.to(device)\n",
        "            model_fp32_prepared(inputs)\n",
        "    model_int8 = convert(model_fp32_prepared)\n",
        "    return model_int8\n",
        "\n",
        "def load_data_from_pickle(file_path):\n",
        "    \"\"\"\n",
        "    Loads image paths and labels from a pickle file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): The path to the pickle file containing the data.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing lists of image paths and their corresponding labels.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        image_paths, labels = pickle.load(file)\n",
        "    return image_paths, labels\n",
        "\n",
        "def load_model(model_path, output_classes=6):\n",
        "    \"\"\"\n",
        "    Loads a trained model from a specified file path.\n",
        "\n",
        "    Parameters:\n",
        "    - model_path (str): The path to the file containing the model's state dictionary.\n",
        "    - output_classes (int): The number of output classes for the model's fully connected layer.\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): The loaded and initialized model.\n",
        "    \"\"\"\n",
        "    model = initialize_model(output_classes)\n",
        "    saved_contents = torch.load(model_path)\n",
        "    state_dict = saved_contents[\"state_dict\"]\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): DynamicQuantizedLinear(in_features=2048, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "    (1): ReLU()\n",
              "    (2): DynamicQuantizedLinear(in_features=512, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class that extends PyTorch's Dataset class for image loading and preprocessing.\n",
        "\n",
        "    Attributes:\n",
        "    - image_paths (list): List of paths to the images.\n",
        "    - labels (list): List of labels corresponding to the images.\n",
        "    - transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset with images and labels.\n",
        "\n",
        "        Parameters:\n",
        "        - image_paths (list): List of paths to the images.\n",
        "        - labels (list): List of labels for the images.\n",
        "        - transform (callable, optional): Optional transform to apply on images.\n",
        "        \"\"\"\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves an image and its label from the dataset at the specified index.\n",
        "\n",
        "        Parameters:\n",
        "        - index (int): Index of the image and label to return.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "\n",
        "        image_path = self.image_paths[index]\n",
        "        # Load the image as a PIL Image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2: Apply Quantization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the device for computation - Quantized models are typically used on CPUs\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load test data from a serialized pickle file\n",
        "test_file = 'full_dataset_segmented_test_data.pkl'\n",
        "test_paths, test_labels = load_data_from_pickle(test_file)\n",
        "\n",
        "# Normalization parameters aligned with those used for pre-trained models\n",
        "normalization = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "# Transformation pipeline for testing data, suitable for EfficientNet and other ImageNet-trained models\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224, antialias=True),  # Resize images to 224x224 pixels, applying antialiasing for image quality\n",
        "    transforms.ToTensor(),                   # Convert images to tensor format suitable for model input\n",
        "    transforms.Normalize(*normalization),    # Apply normalization using predefined mean and std deviation values\n",
        "])\n",
        "\n",
        "# Create a dataset and dataloader for testing\n",
        "test_dataset = CustomDataset(test_paths, test_labels, transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=0)\n",
        "\n",
        "# Load a pre-trained model from a specified path and prepare it for quantization\n",
        "model_path = 'RESNET50_best_model.pth'  # Model path could be adjusted if needed\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Apply static quantization to the model\n",
        "quantized_model = static_quantize(QuantizedResNet(model), test_dataloader, device)\n",
        "quantized_model.to(device)  # Move the quantized model to the designated computing device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 3: Evaluate the Quantized Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, loss_fun, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The neural network model to evaluate.\n",
        "    - dataloader: The DataLoader containing the dataset for evaluation.\n",
        "    - loss_fun: The loss function used to compute the model's loss.\n",
        "    - device: The device (CPU or CUDA) on which the computations will be performed.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple of average loss and accuracy over the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode    test_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed for evaluation\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)  # Move data to the same device as the model\n",
        "\n",
        "            pred = model(x)\n",
        "            _, predicted_classes = torch.max(pred, 1)\n",
        "            correct_predictions = (predicted_classes == y).float()\n",
        "\n",
        "            loss = loss_fun(pred, y.long())  # Ensure consistent data type\n",
        "            val_loss += loss.item()\n",
        "            val_acc += correct_predictions.sum().item() / y.size(0)\n",
        "\n",
        "        val_loss /= len(dataloader)\n",
        "        val_acc /= len(dataloader)\n",
        "\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized Model Test Loss: 0.5621, Test Accuracy: 0.8341\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "test_loss, test_acc = evaluate_model(quantized_model, test_dataloader, loss_fun, device)\n",
        "\n",
        "print(f\"Quantized Model Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
