{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Segment into Train/Validation/Testing + Statisfy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_save_stratified_data(data_dir, save_path):\n",
    "    \"\"\"\n",
    "    Splits the dataset into stratified training, validation, and test sets and saves them to files,\n",
    "    with filenames that include the save_path prefix directly.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dir (str): Directory containing the dataset, organized into subdirectories for each category.\n",
    "    - save_path (str): Base prefix to be used in the filename for saving the dataset splits.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    categories = ['Trash', 'Plastic', 'Paper', 'Metal', 'Glass', 'Cardboard']\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Collect all image paths and their corresponding labels\n",
    "    for label, category in enumerate(categories):\n",
    "        category_dir = os.path.join(data_dir, category)\n",
    "        for file in os.listdir(category_dir):\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                image_paths.append(os.path.join(category_dir, file))\n",
    "                labels.append(label)\n",
    "\n",
    "    # Split data into train and test with stratification (85% train, 15% test)\n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.15, stratify=labels, random_state=0)\n",
    "\n",
    "    # Split train data into train and validation with stratification (remaining 85% train, 15% validation of the train set)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_paths, train_labels, test_size=0.175, stratify=train_labels, random_state=0)  # 0.175 x 0.85 â‰ˆ 0.15\n",
    "\n",
    "    # Save the data splits\n",
    "    with open(f\"{save_path}_train_data.pkl\", 'wb') as f:\n",
    "        pickle.dump((train_paths, train_labels), f)\n",
    "    with open(f\"{save_path}_val_data.pkl\", 'wb') as f:\n",
    "        pickle.dump((val_paths, val_labels), f)\n",
    "    with open(f\"{save_path}_test_data.pkl\", 'wb') as f:\n",
    "        pickle.dump((test_paths, test_labels), f)\n",
    "\n",
    "    print(f\"Data splits saved with prefix '{save_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits saved with prefix 'full_dataset_segmented'\n"
     ]
    }
   ],
   "source": [
    "folder_path = '../../data/dataset-resized' # My local path\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1' # for timm\n",
    "desired_name = \"full_dataset_segmented\"\n",
    "segment_and_save_stratified_data(folder_path, desired_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, double check it worked well and show the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from a pickle file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the pickle file where the dataset is stored.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Two lists containing the image file paths and corresponding labels.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        image_paths, labels = pickle.load(file)\n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "def count_categories(labels):\n",
    "    \"\"\"\n",
    "    Counts the occurrences of each category in a list of labels.\n",
    "\n",
    "    Parameters:\n",
    "    - labels (list of int): List of label indices.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary with counts of each category.\n",
    "    \"\"\"\n",
    "    categories = ['Trash', 'Plastic', 'Paper', 'Metal', 'Glass', 'Cardboard']\n",
    "    counts = {category: 0 for category in categories}\n",
    "    for label in labels:\n",
    "        counts[categories[label]] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Category Counts: {'Trash': 96, 'Plastic': 338, 'Paper': 417, 'Metal': 287, 'Glass': 351, 'Cardboard': 282}\n",
      "Validation Data Category Counts: {'Trash': 20, 'Plastic': 72, 'Paper': 88, 'Metal': 61, 'Glass': 75, 'Cardboard': 60}\n",
      "Test Data Category Counts: {'Trash': 21, 'Plastic': 72, 'Paper': 89, 'Metal': 62, 'Glass': 75, 'Cardboard': 61}\n"
     ]
    }
   ],
   "source": [
    "# Filenames for the dataset splits\n",
    "train_file = desired_name + '_train_data.pkl'\n",
    "val_file = desired_name + '_val_data.pkl'\n",
    "test_file = desired_name + '_test_data.pkl'\n",
    "\n",
    "# Load the datasets\n",
    "train_paths, train_labels = load_dataset(train_file)\n",
    "val_paths, val_labels = load_dataset(val_file)\n",
    "test_paths, test_labels = load_dataset(test_file)\n",
    "\n",
    "# Count categories in each dataset\n",
    "train_counts = count_categories(train_labels)\n",
    "val_counts = count_categories(val_labels)\n",
    "test_counts = count_categories(test_labels)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Data Category Counts:\", train_counts)\n",
    "print(\"Validation Data Category Counts:\", val_counts)\n",
    "print(\"Test Data Category Counts:\", test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Segmment into smaller subsets (In this case, groups of 25, 50, and 100 of each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of size 25 for each category saved to subset_25_data_segmented.pkl\n",
      "Subset of size 50 for each category saved to subset_50_data_segmented.pkl\n",
      "Subset of size 100 for each category saved to subset_100_data_segmented.pkl\n",
      "Data Category Counts for 25 instances of each category: {'Trash': 25, 'Plastic': 25, 'Paper': 25, 'Metal': 25, 'Glass': 25, 'Cardboard': 25}\n",
      "Data Category Counts for 50 instances of each category: {'Trash': 50, 'Plastic': 50, 'Paper': 50, 'Metal': 50, 'Glass': 50, 'Cardboard': 50}\n",
      "Data Category Counts for 100 instances of each category: {'Trash': 96, 'Plastic': 100, 'Paper': 100, 'Metal': 100, 'Glass': 100, 'Cardboard': 100}\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        image_paths, labels = pickle.load(file)\n",
    "    return image_paths, labels\n",
    "\n",
    "def save_data(filename, image_paths, labels):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump((image_paths, labels), file)\n",
    "\n",
    "def create_subsets(image_paths, labels, sizes, prefix):\n",
    "    \"\"\"\n",
    "    Creates subsets of the dataset for different sizes specified, ensuring no duplicates, and saves them.\n",
    "\n",
    "    Parameters:\n",
    "    - image_paths (list of str): Paths to the images in the dataset.\n",
    "    - labels (list of int): Corresponding labels for the images.\n",
    "    - sizes (list of int): Sizes of the subsets to create for each category.\n",
    "    - prefix (str): Prefix for naming the output files.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Group image paths by label\n",
    "    categorized_data = defaultdict(list)\n",
    "    for path, label in zip(image_paths, labels):\n",
    "        categorized_data[label].append(path)\n",
    "\n",
    "    categories = ['Trash', 'Plastic', 'Paper', 'Metal', 'Glass', 'Cardboard']\n",
    "    \n",
    "    # Ensure deterministic randomness, in case we re-run this cell.\n",
    "    random.seed(0)\n",
    "\n",
    "    for size in sizes:\n",
    "        subset_paths = []\n",
    "        subset_labels = []\n",
    "        # Create subset for each category\n",
    "        for label, cat_name in enumerate(categories):\n",
    "            if len(categorized_data[label]) >= size:\n",
    "                random.shuffle(categorized_data[label])\n",
    "                selected_paths = categorized_data[label][:size]\n",
    "            else:\n",
    "                selected_paths = categorized_data[label]  # If not enough data, take all\n",
    "\n",
    "            subset_paths.extend(selected_paths)\n",
    "            subset_labels.extend([label] * len(selected_paths))\n",
    "\n",
    "        # Save the subset to a file\n",
    "        filename = f\"{prefix}_{size}_data_segmented.pkl\"\n",
    "        save_data(filename, subset_paths, subset_labels)\n",
    "        print(f\"Subset of size {size} for each category saved to {filename}\")\n",
    "\n",
    "# Load original training data\n",
    "train_file = 'full_dataset_segmented_train_data.pkl'\n",
    "train_paths, train_labels = load_data(train_file)\n",
    "\n",
    "# Define sizes for the subsets\n",
    "sizes = [25, 50, 100]\n",
    "\n",
    "# Create and save subsets\n",
    "prefix = 'subset'\n",
    "create_subsets(train_paths, train_labels, sizes, prefix)\n",
    "\n",
    "# Loop through each size, load the dataset, and print the category counts\n",
    "for size in sizes:\n",
    "    subset_file = f'{prefix}_{size}_data_segmented.pkl'\n",
    "    paths, labels = load_data(subset_file)\n",
    "    counts = count_categories(labels)\n",
    "    print(f\"Data Category Counts for {size} instances of each category:\", counts)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "squirrelHill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
